{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIa11NfhXSdW"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/henry123-boy/SpaTracker\n",
        "%cd /content/SpaTracker\n",
        "!pip install timm==0.6.7 flow_vis\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!mkdir /content/checkpoints\n",
        "!gdown -O /content/checkpoints/spaT_final.pth 18YlG_rgrHcJ7lIYQWfRz_K669z6FdmUX\n",
        "!gdown -O /content/SpaTracker/assets/butterfly.mp4 1BDtvfrvbzEFY84XJPp62Dq1PujIpbOK_\n",
        "!gdown -O /content/SpaTracker/assets/butterfly.png 1hlAGFony7LzpLcEAoGLiNaY3zxfiN_bW\n",
        "!gdown -O /content/SpaTracker/assets/sintel_bandage.mp4 1iL5Qs5ea8r9nFwgVC6fusFyBfFQDICyo\n",
        "!gdown -O /content/SpaTracker/assets/sintel_bandage.png 1_cL3m_1bW6aFwhxRPr_vGkdizwJuObbH\n",
        "\n",
        "!mkdir -p /content/models/monoD/zoeDepth/ckpts\n",
        "!wget -O /content/models/monoD/zoeDepth/ckpts/dpt_beit_large_384.pt https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_384.pt\n",
        "!wget -O /content/models/monoD/zoeDepth/ckpts/ZoeD_M12_K.pt https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_K.pt\n",
        "!wget -O /content/models/monoD/zoeDepth/ckpts/ZoeD_M12_NK.pt https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_NK.pt\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/DepthAnything/Depth-Anything-V2\n",
        "%cd /content/Depth-Anything-V2\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!mkdir /content/checkpoints\n",
        "!wget -O /content/checkpoints/depth_anything_v2_vitl.pth https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "pBkkpRhoXpIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "from easydict import EasyDict as edict\n",
        "from base64 import b64encode\n",
        "import importlib\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "\n",
        "\n",
        "%cd /content/SpaTracker/\n",
        "\n",
        "from models.spatracker.predictor import SpaTrackerPredictor\n",
        "from models.spatracker.utils.visualizer import Visualizer\n",
        "from models.monoD.zoeDepth.models.builder import build_model\n",
        "from models.monoD.zoeDepth.utils.config import get_config\n",
        "\n",
        "%cd /content/Depth-Anything-V2\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPY8EW9pZTYI",
        "outputId": "dfa7c91f-3b5e-42e4-9e6d-3bb19754d6bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SpaTracker\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:dinov2:xFormers not available\n",
            "WARNING:dinov2:xFormers not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Depth-Anything-V2\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_video(path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(\"Error opening video file\")\n",
        "\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    cap.release()\n",
        "    return np.stack(frames)\n",
        "\n",
        "def load_video(vid_path, downsample, fps):\n",
        "    video = read_video(vid_path)\n",
        "    video = torch.from_numpy(video).permute(0, 3, 1, 2).unsqueeze(0).float()\n",
        "    video = F.interpolate(video[0], scale_factor=downsample, mode='bilinear', align_corners=True).unsqueeze(0)\n",
        "    idx = torch.arange(0, video.shape[1], fps).long()\n",
        "    return video[:, idx]\n",
        "\n",
        "def load_segmentation_mask(seg_path, H, W):\n",
        "    if os.path.exists(seg_path):\n",
        "        segm_mask = np.array(Image.open(seg_path))\n",
        "    else:\n",
        "        segm_mask = np.ones((H, W), dtype=np.uint8)\n",
        "        print(\"No segmentation mask provided. Computing tracks in whole image.\")\n",
        "\n",
        "    if segm_mask.ndim == 3:\n",
        "        segm_mask = (segm_mask[..., :3].mean(axis=-1) > 0).astype(np.uint8)\n",
        "\n",
        "    return cv2.resize(segm_mask, (W, H), interpolation=cv2.INTER_NEAREST)"
      ],
      "metadata": {
        "id": "CKfOpn6VsFo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(video, pred_tracks, pred_visibility, outdir, vid_name, fps_vis, len_track, point_size):\n",
        "    vis = Visualizer(save_dir=outdir, grayscale=True,\n",
        "                     fps=fps_vis, pad_value=0, linewidth=point_size,\n",
        "                     tracks_leave_trace=len_track)\n",
        "    return vis.visualize(\n",
        "        video=video,\n",
        "        tracks=pred_tracks[..., :2],\n",
        "        visibility=pred_visibility,\n",
        "        filename=f\"{vid_name}_spatracker\"\n",
        "    )"
      ],
      "metadata": {
        "id": "KMIR7H9JsLXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(video, video_vis, pred_tracks, outdir, vid_name, model_name):\n",
        "    img0 = video_vis[0,0].permute(1,2,0).detach().cpu().numpy()\n",
        "    cv2.imwrite(os.path.join(outdir, f'{vid_name}_ref_query.png'), img0[:,:,::-1])\n",
        "\n",
        "    tracks_vis = pred_tracks[0].detach().cpu().numpy()\n",
        "    np.save(os.path.join(outdir, f'{vid_name}_{model_name}_tracks.npy'), tracks_vis)\n",
        "\n",
        "    wide_list = [wide[0].permute(1, 2, 0).cpu().numpy() for wide in video.unbind(1)]\n",
        "    clip = ImageSequenceClip(wide_list, fps=60)\n",
        "    save_path = os.path.join(outdir, f'{vid_name}_vid.mp4')\n",
        "    clip.write_videofile(save_path, codec=\"libx264\", fps=25, logger=None)\n",
        "    print(f\"Original Video saved to {save_path}\")\n",
        "\n",
        "def save_3d_trajectories(pred_tracks, video, outdir, vid_name):\n",
        "    T, N, _ = pred_tracks[0].shape\n",
        "    H, W = video[0].shape[-2:]\n",
        "    xyzt = pred_tracks[0].cpu().numpy()\n",
        "\n",
        "    intr = np.array([[W, 0.0, W//2],\n",
        "                     [0.0, W, H//2],\n",
        "                     [0.0, 0.0, 1.0]])\n",
        "\n",
        "    xyztVis = xyzt.copy()\n",
        "    xyztVis[..., 2] = 1.0\n",
        "    xyztVis = np.linalg.inv(intr[None, ...]) @ xyztVis.reshape(-1, 3, 1)\n",
        "    xyztVis = xyztVis.reshape(T, -1, 3)\n",
        "    xyztVis[..., 2] *= xyzt[..., 2]\n",
        "\n",
        "    pred_tracks2d = pred_tracks[0][:, :, :2]\n",
        "    pred_tracks2dNm = pred_tracks2d.clone()\n",
        "    pred_tracks2dNm[..., 0] = 2*(pred_tracks2dNm[..., 0] / W - 0.5)\n",
        "    pred_tracks2dNm[..., 1] = 2*(pred_tracks2dNm[..., 1] / H - 0.5)\n",
        "\n",
        "    color_interp = torch.nn.functional.grid_sample(video[0], pred_tracks2dNm[:,:,None,:], align_corners=True)\n",
        "    color_interp = color_interp[:, :, :, 0].permute(0,2,1).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    colored_pts = np.concatenate([xyztVis, color_interp], axis=-1)\n",
        "    np.save(f'{outdir}/{vid_name}_3d.npy', colored_pts)\n",
        "    print(f\"3D colored tracks saved to {outdir}/{vid_name}_3d.npy\")"
      ],
      "metadata": {
        "id": "_zdPZZnksOUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_environment(root, vid_name, outdir):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    vid_path = os.path.join(root, f'{vid_name}.mp4')\n",
        "    seg_path = os.path.join(root, f'{vid_name}.png')\n",
        "    return vid_path, seg_path\n",
        "\n",
        "def load_and_process_data(vid_path, seg_path, downsample, fps):\n",
        "    video = load_video(vid_path, downsample, fps)\n",
        "    _, _, _, H, W = video.shape\n",
        "    segm_mask = load_segmentation_mask(seg_path, H, W)\n",
        "    return video, segm_mask\n",
        "\n",
        "def save_initial_images(video, segm_mask, outdir, vid_name):\n",
        "    img0 = video[0, 0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "    cv2.imwrite(os.path.join(outdir, f'{vid_name}_ref.png'), img0[:, :, ::-1])\n",
        "    cv2.imwrite(os.path.join(outdir, f'{vid_name}_seg.png'), segm_mask * 255)\n",
        "\n",
        "def initialize_models(checkpoint_path, interp_shape, seq_length, device):\n",
        "    spatracker_predictor = SpaTrackerPredictor(\n",
        "        checkpoint=checkpoint_path,\n",
        "        interp_shape=interp_shape,\n",
        "        seq_length=seq_length\n",
        "    ).to(device)\n",
        "    monodepth_model = MonoDEst().model\n",
        "    monodepth_model.eval()\n",
        "    return spatracker_predictor, monodepth_model\n",
        "\n",
        "def process_video(spatracker_predictor, monodepth_model, video, segm_mask, query_frame, seq_length):\n",
        "    pred_tracks, pred_visibility, T_Firsts = spatracker_predictor(\n",
        "        video, video_depth=None, grid_size=40, backward_tracking=False,\n",
        "        depth_predictor=monodepth_model, grid_query_frame=query_frame,\n",
        "        segm_mask=torch.from_numpy(segm_mask).unsqueeze(0).unsqueeze(0),\n",
        "        wind_length=seq_length\n",
        "    )\n",
        "\n",
        "    msk_query = (T_Firsts == query_frame)\n",
        "    pred_tracks = pred_tracks[:, :, msk_query.squeeze()]\n",
        "    pred_visibility = pred_visibility[:, :, msk_query.squeeze()]\n",
        "    return pred_tracks, pred_visibility"
      ],
      "metadata": {
        "id": "pFkDpNGDtxhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DepthAnythingV2Wrapper(nn.Module):\n",
        "    def __init__(self, encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024]):\n",
        "        super(DepthAnythingV2Wrapper, self).__init__()\n",
        "        self.model = DepthAnythingV2(encoder=encoder, features=features, out_channels=out_channels)\n",
        "        self.model.load_state_dict(torch.load('/content/checkpoints/depth_anything_v2_vitl.pth', map_location='cpu'))\n",
        "        self.model = self.model.to(device).eval()\n",
        "\n",
        "    def infer(self, rgbs):\n",
        "        with torch.no_grad():\n",
        "            batch_size, channels, height, width = rgbs.shape\n",
        "            depth_maps = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                img = rgbs[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img = (img * 255).astype(np.uint8)\n",
        "                depth = self.model.infer_image(img)\n",
        "\n",
        "                depth_tensor = torch.from_numpy(depth).unsqueeze(0).unsqueeze(0)\n",
        "                depth_maps.append(depth_tensor)\n",
        "\n",
        "            return torch.cat(depth_maps, dim=0).to(rgbs.device)\n",
        "\n",
        "class MonoDEst(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MonoDEst, self).__init__()\n",
        "        self.model = self._build_model()\n",
        "        self.metric3d = build_model(get_config(\"zoedepth_nk\", \"infer\")).to(device).eval()\n",
        "\n",
        "    def _build_model(self):\n",
        "        return DepthAnythingV2Wrapper(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\n",
        "\n",
        "    def infer(self, rgbs, scale=None, shift=None):\n",
        "        depth_map = self.model.infer(rgbs)\n",
        "        metric_dp = self.metric3d.infer(rgbs[:20])\n",
        "        metric_dp_inv = 1 / metric_dp\n",
        "        dp_0_rel = depth_map[:20]\n",
        "        scale, shift = np.polyfit(dp_0_rel.view(-1).cpu().numpy(),\n",
        "                                  metric_dp_inv.view(-1).cpu().numpy(), 1)\n",
        "        depth_map = depth_map * scale + shift\n",
        "        return (1 / depth_map).clamp(0.01, 65)"
      ],
      "metadata": {
        "id": "gmTXwyuNpSdp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "root = '/content/SpaTracker/assets'\n",
        "vid_name = 'butterfly'\n",
        "outdir = './vis_results'\n",
        "downsample = 1\n",
        "fps = 1\n",
        "len_track = 1\n",
        "fps_vis = 15\n",
        "query_frame = 0\n",
        "point_size = 3\n",
        "seq_length = 12\n",
        "checkpoint_path = '/content/checkpoints/spaT_final.pth'\n",
        "interp_shape = (384, 512)\n",
        "\n",
        "vid_path, seg_path = setup_environment(root, vid_name, outdir)\n",
        "video, segm_mask = load_and_process_data(vid_path, seg_path, downsample, fps)\n",
        "save_initial_images(video, segm_mask, outdir, vid_name)\n",
        "\n",
        "spatracker_predictor, monodepth_model = initialize_models(checkpoint_path, interp_shape, seq_length, device)\n",
        "\n",
        "video = video.to(device)\n",
        "pred_tracks, pred_visibility = process_video(spatracker_predictor, monodepth_model, video, segm_mask, query_frame, seq_length)\n",
        "\n",
        "video_vis = visualize_results(video, pred_tracks, pred_visibility, outdir, vid_name, fps_vis, len_track, point_size)\n",
        "save_results(video, video_vis, pred_tracks, outdir, vid_name, 'spatracker')\n",
        "save_3d_trajectories(pred_tracks, video, outdir, vid_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZahLJ3iFZ4dS",
        "outputId": "4f0f53a3-72ac-446f-8c22-66e85d559dae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_size [384, 512]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params passed to Resize transform:\n",
            "\twidth:  512\n",
            "\theight:  384\n",
            "\tresize_target:  True\n",
            "\tkeep_aspect_ratio:  True\n",
            "\tensure_multiple_of:  32\n",
            "\tresize_method:  minimal\n",
            "Using pretrained resource local::./models/monoD/zoeDepth/ckpts/ZoeD_M12_NK.pt\n",
            "Loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/cupy/cuda/compiler.py:464: UserWarning: cupy.cuda.compile_with_cache has been deprecated in CuPy v10, and will be removed in the future. Use cupy.RawModule or cupy.RawKernel instead.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for inference:  77.01509428024292\n",
            "Video saved to ./vis_results/butterfly_spatracker_pred_track.mp4\n",
            "Original Video saved to ./vis_results/butterfly_vid.mp4\n",
            "3D colored tracks saved to ./vis_results/butterfly_3d.npy\n"
          ]
        }
      ]
    }
  ]
}