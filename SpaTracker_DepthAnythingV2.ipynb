{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "hK8baryVyfN2",
        "54AlGztgyisI",
        "ghmu21TUANSZ",
        "5aq3y63V0R2U"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports and Helper Functions"
      ],
      "metadata": {
        "id": "hK8baryVyfN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installations"
      ],
      "metadata": {
        "id": "54AlGztgyisI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIa11NfhXSdW",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install Spatial Tracker and Download Zeo Depth Model\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/henry123-boy/SpaTracker\n",
        "%cd /content/SpaTracker\n",
        "!pip install av\n",
        "!pip install timm==0.6.7 flow_vis\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!mkdir /content/checkpoints\n",
        "!gdown -O /content/checkpoints/spaT_final.pth 18YlG_rgrHcJ7lIYQWfRz_K669z6FdmUX\n",
        "\n",
        "!mkdir -p /content/models/monoD/zoeDepth/ckpts\n",
        "!wget -O /content/models/monoD/zoeDepth/ckpts/dpt_beit_large_384.pt https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_384.pt\n",
        "!wget -O /content/models/monoD/zoeDepth/ckpts/ZoeD_M12_K.pt https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_K.pt\n",
        "!wget -O /content/models/monoD/zoeDepth/ckpts/ZoeD_M12_NK.pt https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_NK.pt\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Depth Anything V2\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/DepthAnything/Depth-Anything-V2\n",
        "%cd /content/Depth-Anything-V2\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "!mkdir /content/checkpoints\n",
        "!wget -O /content/checkpoints/depth_anything_v2_vitl.pth https://huggingface.co/depth-anything/Depth-Anything-V2-Large/resolve/main/depth_anything_v2_vitl.pth?download=true\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "pBkkpRhoXpIx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Some Example\n",
        "\n",
        "!gdown -O /content/SpaTracker/assets/butterfly.mp4 1BDtvfrvbzEFY84XJPp62Dq1PujIpbOK_\n",
        "!gdown -O /content/SpaTracker/assets/butterfly.png 1hlAGFony7LzpLcEAoGLiNaY3zxfiN_bW\n",
        "!gdown -O /content/SpaTracker/assets/sintel_bandage.mp4 1iL5Qs5ea8r9nFwgVC6fusFyBfFQDICyo\n",
        "!gdown -O /content/SpaTracker/assets/sintel_bandage.png 1_cL3m_1bW6aFwhxRPr_vGkdizwJuObbH"
      ],
      "metadata": {
        "cellView": "form",
        "id": "X_gixxlly-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "ghmu21TUANSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, gc\n",
        "from typing import Optional\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "\n",
        "%cd /content/SpaTracker/\n",
        "\n",
        "from models.spatracker.predictor import SpaTrackerPredictor\n",
        "from models.spatracker.utils.visualizer import Visualizer\n",
        "from models.monoD.zoeDepth.models.builder import build_model\n",
        "from models.monoD.zoeDepth.utils.config import get_config\n",
        "\n",
        "%cd /content/Depth-Anything-V2\n",
        "\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPY8EW9pZTYI",
        "outputId": "ba8c3585-05c7-4ede-d62d-f6bcb1d51700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SpaTracker\n",
            "/content/Depth-Anything-V2\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helpers"
      ],
      "metadata": {
        "id": "5aq3y63V0R2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Memory Helpers\n",
        "\n",
        "def free_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.ipc_collect()\n",
        "        torch.cuda.synchronize()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Imb29UglLXJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Video Helpers\n",
        "\n",
        "def load_video(\n",
        "    vid_path: str,\n",
        "    downsample: float = 1.0,\n",
        "    frame_interval: int = 1,\n",
        "    max_frames: Optional[int] = None\n",
        ") -> torch.Tensor:\n",
        "    cap = cv2.VideoCapture(vid_path)\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(f\"Error opening video file: {vid_path}\")\n",
        "\n",
        "    try:\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        if max_frames:\n",
        "            total_frames = min(total_frames, max_frames)\n",
        "\n",
        "        frames = []\n",
        "        for i in range(0, total_frames, frame_interval):\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame = process_frame(frame, downsample)\n",
        "            frames.append(frame)\n",
        "\n",
        "        if not frames:\n",
        "            raise RuntimeError(\"No frames could be read from the video\")\n",
        "\n",
        "        return torch.cat(frames, dim=0)\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "def process_frame(frame: np.ndarray, downsample: float) -> torch.Tensor:\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float().unsqueeze(0)\n",
        "\n",
        "    if downsample != 1.0:\n",
        "        frame_tensor = F.interpolate(\n",
        "            frame_tensor,\n",
        "            scale_factor=downsample,\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "    return frame_tensor\n",
        "\n",
        "def load_segmentation_mask(seg_path, H, W):\n",
        "    if os.path.exists(seg_path):\n",
        "        segm_mask = np.array(Image.open(seg_path))\n",
        "    else:\n",
        "        segm_mask = np.ones((H, W), dtype=np.uint8)\n",
        "        print(\"No segmentation mask provided. Computing tracks in whole image.\")\n",
        "\n",
        "    if segm_mask.ndim == 3:\n",
        "        segm_mask = (segm_mask[..., :3].mean(axis=-1) > 0).astype(np.uint8)\n",
        "\n",
        "    return cv2.resize(segm_mask, (W, H), interpolation=cv2.INTER_NEAREST)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CKfOpn6VsFo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualization Helpers\n",
        "\n",
        "def visualize_results(video, pred_tracks, pred_visibility, outdir, vid_name, fps_vis, len_track, point_size):\n",
        "    vis = Visualizer(save_dir=outdir, grayscale=True,\n",
        "                     fps=fps_vis, pad_value=0, linewidth=point_size,\n",
        "                     tracks_leave_trace=len_track)\n",
        "    return vis.visualize(\n",
        "        video=video,\n",
        "        tracks=pred_tracks[..., :2],\n",
        "        visibility=pred_visibility,\n",
        "        filename=f\"{vid_name}_spatracker\"\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KMIR7H9JsLXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Saving helpers\n",
        "\n",
        "def save_results(video, video_vis, pred_tracks, outdir, vid_name, model_name):\n",
        "    img0 = video_vis[0,0].permute(1,2,0).detach().cpu().numpy()\n",
        "    cv2.imwrite(os.path.join(outdir, f'{vid_name}_ref_query.png'), img0[:,:,::-1])\n",
        "\n",
        "    tracks_vis = pred_tracks[0].detach().cpu().numpy()\n",
        "    np.save(os.path.join(outdir, f'{vid_name}_{model_name}_tracks.npy'), tracks_vis)\n",
        "\n",
        "    wide_list = [wide[0].permute(1, 2, 0).cpu().numpy() for wide in video.unbind(1)]\n",
        "    clip = ImageSequenceClip(wide_list, fps=60)\n",
        "    save_path = os.path.join(outdir, f'{vid_name}_vid.mp4')\n",
        "    clip.write_videofile(save_path, codec=\"libx264\", fps=25, logger=None)\n",
        "    print(f\"Original Video saved to {save_path}\")\n",
        "\n",
        "def save_3d_trajectories(pred_tracks, video, outdir, vid_name):\n",
        "    T, N, _ = pred_tracks[0].shape\n",
        "    H, W = video[0].shape[-2:]\n",
        "    xyzt = pred_tracks[0].cpu().numpy()\n",
        "\n",
        "    intr = np.array([[W, 0.0, W//2],\n",
        "                     [0.0, W, H//2],\n",
        "                     [0.0, 0.0, 1.0]])\n",
        "\n",
        "    xyztVis = xyzt.copy()\n",
        "    xyztVis[..., 2] = 1.0\n",
        "    xyztVis = np.linalg.inv(intr[None, ...]) @ xyztVis.reshape(-1, 3, 1)\n",
        "    xyztVis = xyztVis.reshape(T, -1, 3)\n",
        "    xyztVis[..., 2] *= xyzt[..., 2]\n",
        "\n",
        "    pred_tracks2d = pred_tracks[0][:, :, :2]\n",
        "    pred_tracks2dNm = pred_tracks2d.clone()\n",
        "    pred_tracks2dNm[..., 0] = 2*(pred_tracks2dNm[..., 0] / W - 0.5)\n",
        "    pred_tracks2dNm[..., 1] = 2*(pred_tracks2dNm[..., 1] / H - 0.5)\n",
        "\n",
        "    color_interp = torch.nn.functional.grid_sample(video[0], pred_tracks2dNm[:,:,None,:], align_corners=True)\n",
        "    color_interp = color_interp[:, :, :, 0].permute(0,2,1).cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    colored_pts = np.concatenate([xyztVis, color_interp], axis=-1)\n",
        "    np.save(f'{outdir}/{vid_name}_3d.npy', colored_pts)\n",
        "    print(f\"3D colored tracks saved to {outdir}/{vid_name}_3d.npy\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_zdPZZnksOUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation"
      ],
      "metadata": {
        "id": "CoFr7F_N0oPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Models"
      ],
      "metadata": {
        "id": "jfrchULL0r1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Depth Model\n",
        "\n",
        "class DepthAnythingV2Wrapper(nn.Module):\n",
        "    def __init__(self, encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024]):\n",
        "        super(DepthAnythingV2Wrapper, self).__init__()\n",
        "        self.model = DepthAnythingV2(encoder=encoder, features=features, out_channels=out_channels)\n",
        "        self.model.load_state_dict(torch.load('/content/checkpoints/depth_anything_v2_vitl.pth', map_location='cpu'))\n",
        "        self.model = self.model.to(device).eval()\n",
        "\n",
        "    def infer(self, rgbs):\n",
        "        with torch.no_grad():\n",
        "            batch_size, channels, height, width = rgbs.shape\n",
        "            depth_maps = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                img = rgbs[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img = (img * 255).astype(np.uint8)\n",
        "                depth = self.model.infer_image(img)\n",
        "\n",
        "                depth_tensor = torch.from_numpy(depth).unsqueeze(0).unsqueeze(0)\n",
        "                depth_maps.append(depth_tensor)\n",
        "\n",
        "            return torch.cat(depth_maps, dim=0).to(rgbs.device)\n",
        "\n",
        "class MonoDEst(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MonoDEst, self).__init__()\n",
        "        self.model = self._build_model()\n",
        "        self.metric3d = build_model(get_config(\"zoedepth_nk\", \"infer\")).to(device).eval()\n",
        "\n",
        "    def _build_model(self):\n",
        "        return DepthAnythingV2Wrapper(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\n",
        "\n",
        "    def infer(self, rgbs, scale=None, shift=None):\n",
        "        depth_map = self.model.infer(rgbs)\n",
        "        metric_dp = self.metric3d.infer(rgbs[:20])\n",
        "        metric_dp_inv = 1 / metric_dp\n",
        "        dp_0_rel = depth_map[:20]\n",
        "        scale, shift = np.polyfit(dp_0_rel.view(-1).cpu().numpy(),\n",
        "                                  metric_dp_inv.view(-1).cpu().numpy(), 1)\n",
        "        depth_map = depth_map * scale + shift\n",
        "        return (1 / depth_map).clamp(0.01, 65)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gmTXwyuNpSdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference"
      ],
      "metadata": {
        "id": "H6Fm-gcJ02BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_environment(root, vid_name, outdir):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    vid_path = os.path.join(root, f'{vid_name}.mp4')\n",
        "    seg_path = os.path.join(root, f'{vid_name}.png')\n",
        "    return vid_path, seg_path\n",
        "\n",
        "def load_and_process_data(vid_path, seg_path, downsample, fps):\n",
        "    video = load_video(vid_path, downsample, frame_interval)\n",
        "    _, _, H, W = video.shape\n",
        "    segm_mask = load_segmentation_mask(seg_path, H, W)\n",
        "    return video, segm_mask\n",
        "\n",
        "def save_initial_images(video, segm_mask, outdir, vid_name):\n",
        "    img0 = video[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "    cv2.imwrite(os.path.join(outdir, f'{vid_name}_ref.png'), img0[:, :, ::-1])\n",
        "    cv2.imwrite(os.path.join(outdir, f'{vid_name}_seg.png'), segm_mask * 255)\n",
        "\n",
        "def initialize_models(interp_shape, seq_length, device):\n",
        "    spatracker_predictor = SpaTrackerPredictor(\n",
        "        checkpoint='/content/checkpoints/spaT_final.pth',\n",
        "        interp_shape=interp_shape,\n",
        "        seq_length=seq_length\n",
        "    ).to(device)\n",
        "    monodepth_model = MonoDEst().model\n",
        "    monodepth_model.eval()\n",
        "    return spatracker_predictor, monodepth_model\n",
        "\n",
        "def process_video(spatracker_predictor, monodepth_model, video, segm_mask, query_frame, seq_length):\n",
        "    pred_tracks, pred_visibility, T_Firsts = spatracker_predictor(\n",
        "        video, video_depth=None, grid_size=40, backward_tracking=False,\n",
        "        depth_predictor=monodepth_model, grid_query_frame=query_frame,\n",
        "        segm_mask=torch.from_numpy(segm_mask).unsqueeze(0).unsqueeze(0),\n",
        "        wind_length=seq_length\n",
        "    )\n",
        "\n",
        "    msk_query = (T_Firsts == query_frame)\n",
        "    pred_tracks = pred_tracks.cpu()[:, :, msk_query.squeeze()]\n",
        "    pred_visibility = pred_visibility.cpu()[:, :, msk_query.squeeze()]\n",
        "    return pred_tracks, pred_visibility"
      ],
      "metadata": {
        "id": "pFkDpNGDtxhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "root = '/content/SpaTracker/assets'\n",
        "vid_name = 'butterfly'\n",
        "outdir = './vis_results'\n",
        "downsample = 1\n",
        "frame_interval = 2\n",
        "len_track = 3\n",
        "fps_vis = 15\n",
        "query_frame = 0\n",
        "point_size = 3\n",
        "seq_length = 12\n",
        "interp_shape = (384, 512)\n",
        "\n",
        "vid_path, seg_path = setup_environment(root, vid_name, outdir)\n",
        "video, segm_mask = load_and_process_data(vid_path, seg_path, downsample, frame_interval)\n",
        "save_initial_images(video, segm_mask, outdir, vid_name)\n",
        "\n",
        "free_memory()\n",
        "\n",
        "spatracker_predictor, monodepth_model = initialize_models(interp_shape, seq_length, device)\n",
        "\n",
        "free_memory()\n",
        "\n",
        "video = video.unsqueeze(0).to(device)\n",
        "pred_tracks, pred_visibility = process_video(spatracker_predictor, monodepth_model, video, segm_mask, query_frame, seq_length)\n",
        "video = video.cpu()\n",
        "\n",
        "free_memory()\n",
        "\n",
        "video_vis = visualize_results(video, pred_tracks, pred_visibility, outdir, vid_name, fps_vis, len_track, point_size)\n",
        "save_results(video, video_vis, pred_tracks, outdir, vid_name, 'spatracker')\n",
        "save_3d_trajectories(pred_tracks, video, outdir, vid_name)\n",
        "\n",
        "del video, segm_mask, pred_tracks, pred_visibility, video_vis\n",
        "free_memory()\n",
        "clear_gpu_memory()"
      ],
      "metadata": {
        "id": "ZahLJ3iFZ4dS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}